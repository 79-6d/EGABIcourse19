# Environmental data

## Bowerbird/blueant

Very commonly, we want to know about the environmental conditions at our points of interest. For the remote and vast Southern Ocean these data typically come from satellite or model sources. Some data centres provide extraction tools that will pull out a subset of data to suit your requirements, but often it makes more sense to cache entire data collections locally first and then work with them from there.

[bowerbird](https://github.com/AustralianAntarcticDivision/bowerbird) provides a framework for downloading data files to a local collection, and keeping it up to date. The companion [blueant](https://github.com/AustralianAntarcticDivision/blueant) package provides a suite of definitions for Southern Ocean and Antarctic data sources that can be used with `bowerbird`. It encompasses data such as sea ice, bathymetry and land topography, oceanography, and atmospheric reanalysis and weather predictions, from providers such as NASA, NOAA, Copernicus, NSIDC, and Ifremer.

Why might you want to maintain local copies of entire data sets, instead of just fetching subsets of data from providers as needed?

- many analyses make use of data from a variety of providers (in which case there may not be dynamic extraction tools for all of them),
- analyses might need to crunch through a whole collection of data in order to calculate appropriate statistics (temperature anomalies with respect to a long-term mean, for example),
- different parts of the same data set are used in different analyses, in which case making one copy of the whole thing may be easier to manage than having different subsets for different projects,
- a common suite of data are routinely used by a local research community, in which case it makes more sense to keep a local copy for everyone to use, rather than multiple copies being downloaded by different individuals.

In these cases, maintaining local copies of a range of data from third-party providers can be extremely beneficial, especially if that collection is hosted with a fast connection to local compute resources (virtual machines or high-performance computational facilities).


Install from GitHub:

```{r ed_install_pkg, eval = FALSE}
remotes::install_github("AustralianAntarcticDivision/blueant")
```

And load the package before use.

```{r ed1}
library(blueant)
```
### Available data sets

First, we can see the available data sets via the `sources` function.

```{r ed2, cache = TRUE}
srcs <- blueant::sources()
## the names of the first few
head(srcs$name)
## the full details of the first one
srcs[1, ]
```

### Usage

Choose a directory into which to download the data. Usually this would be a persistent directory on your machine so that data sets downloaded in one session would remain available for use in later sessions, and not need re-downloading. A persistent directory could be something like `c:\data\` (on Windows), or you could use the `rappdirs` package (the `user_cache_dir` function) to suggest a suitable directory (cross-platform).

```{r ed0hidden, echo = FALSE, results = "asis"}
if (grepl("ben_ray", tempdir())) {
  cat("Here we'll use the `c:/data/cache` directory:\n")
} else {
  cat("Here we'll use a temporary directory:\n")
}
```

```{r ed0a, include = grepl("ben_ray", tempdir())}
my_data_directory <-  "c:/data/cache"
```
```{r ed0b, include = !grepl("ben_ray", tempdir())}
my_data_directory <-  tempdir()
```

Select the data source that we want:

```{r ed4, cache = TRUE}
data_source <- sources("Southern Ocean marine environmental data")
```

Note that it's a good idea to check the dataset size before downloading it, as some are quite large! (Though if you are running the download interactively, it will ask you before downloading a large data set).

```{r ed5, cache = TRUE}
data_source$collection_size ## size in GB
```

And fetch the data:
```{r ed6, cache = TRUE}
result <- bb_get(data_source, local_file_root = my_data_directory, verbose = TRUE)
```

Now we have a local copy of our data. The sync can be run daily so that the local collection is always up to date - it will only download new files, or files that have changed since the last download. For more information on `bowerbird`, see the [package vignette](https://ropensci.github.io/bowerbird/articles/bowerbird.html).

The `result` object holds information about the data that we downloaded:

```{r ed7, cache = TRUE}
result
```

The `result$files` element tells us about the files:
```{r ed8, cache = TRUE}
head(result$files[[1]])
```

These particular files are netCDF, and so could be read using e.g. the `raster` or `ncdf4` packages. However, different data from different providers will be different in terms of grids, resolutions, projections, variable-naming conventions, and other facets, which tends to complicate these operations. In the next section we'll look at the `raadtools` package, which provides a set of tools for doing common operations on these types of data.


## RAADtools

[`raadtools`](https://github.com/AustralianAntarcticDivision/raadtools) is suite of functions that provide consistent access to a range of environmental and similar data, and tools for working with them. It is designed to work data collections maintained by the `bowerbird`/`blueant` packages, and builds on R's existing ecosystem of packages for working with spatial, raster, and multidimensional data.


We'll focus on two sources of environmental data: sea ice and water depth. Note that water depth does not change with time but sea ice is highly dynamic, so we will want to know what the ice conditions are like on a day-to-day basis.

Download daily sea ice data (from 2013 only), and the ETOPO2 bathymetric data set. ETOPO2 is somewhat dated and low resolution compared to more recent data, but will do as a small dataset for demo purposes:

``` r
library(blueant)
src <- bind_rows(
    sources("NSIDC SMMR-SSM/I Nasateam sea ice concentration", hemisphere = "south",
            time_resolutions = "day", years = 2013),
    sources("ETOPO2 bathymetry"))
result <- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE,
                 confirm = NULL)
```

    ##  
    ## Sat Nov 10 01:00:48 2018 
    ## Synchronizing dataset: NSIDC SMMR-SSM/I Nasateam sea ice concentration 
    ##  
    ##  [... output truncated]

Now we have local copies of those data files. The sync can be run daily so that the local collection is always up to date - it will only download new files, or files that have changed since the last download. For more information on `bowerbird`, see the [package vignette](https://ropensci.github.io/bowerbird/articles/bowerbird.html).

Details of the files can be found in the `result` object, and those files could now be read with packages such as `raster`. However, we are collecting data from a range of sources, and so they will be different in terms of their grids, resolutions, projections, and variable-naming conventions, which tends to complicate these operations. In the next section we'll look at the `raadtools` package, which provides a set of tools for doing common operations on these types of data.

#### Using those environmental data: raadtools

[`raadtools`](https://github.com/AustralianAntarcticDivision/raadtools) is suite of functions that provide consistent access to a range of environmental and similar data, and tools for working with them. It is designed to work data collections maintained by the `bowerbird`/`blueant` packages.

Load the package and tell it where our data collection has been stored:

``` r
library(raadtools)
set_data_roots(my_data_dir)
```

Define our spatial region of interest and extract the bathymetry data from this region, using the ETOPO2 files we just downloaded:

``` r
roi <- round(c(range(ele$lon), range(ele$lat)) + c(-2, 2, -2, 2))
bx <- readtopo("etopo2", xylim = roi)
```

And now we can make a simple plot of our our track superimposed on the bathymetry:

``` r
plot(bx)
lines(ele$lon, ele$lat)
```

<img src="/img/blog-images/2018-11-13-antarctic/plotbathy-1.png" style="display: block; margin: auto;" />

We can also extract the depth values along our track using the `extract()` function in `raadtools`:

``` r
ele$depth <- extract(readtopo, ele[, c("lon", "lat")], topo = "etopo2")
```

Plot the histogram of depth values, showing that most of the track points are located in relatively shallow waters:

``` r
ggplot(ele, aes(depth)) + geom_histogram(bins = 100) + theme_bw()
```

<img src="/img/blog-images/2018-11-13-antarctic/histbathy-1.png" style="display: block; margin: auto;" />

This type of extraction will also work with time-varying data â€” for example, we can extract the sea-ice conditions along our track, based on each track point's location and time:

``` r
ele$ice <- extract(readice, ele[, c("lon", "lat", "date")])
## points outside the ice grid will have missing ice values, so fill them with zeros
ele$ice[is.na(ele$ice)] <- 0
ggplot(ele, aes(date, ice, colour = lat)) + geom_path() + theme_bw()
```

<img src="/img/blog-images/2018-11-13-antarctic/temp2-1.png" style="display: block; margin: auto;" />
